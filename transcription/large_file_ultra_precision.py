#!/usr/bin/env python3
"""
Large File Ultra Precision Processor

å¤§è¦æ¨¡ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ã®æœ€é«˜å“è³ªå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ :
- large-v3ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨
- è©±è€…èªè­˜çµ±åˆ
- ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å‡¦ç†ï¼ˆè¤‡æ•°ãƒ¢ãƒ‡ãƒ«ï¼‰ãªã—ï¼ˆå‡¦ç†æ™‚é–“çŸ­ç¸®ï¼‰
- é«˜å“è³ªã§ã‚‚å®Ÿç”¨çš„ãªå‡¦ç†æ™‚é–“
"""

import os
import sys
import time
import json
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
import tempfile

import librosa
import numpy as np

from .transcriber import Transcriber
from .post_processor import TranscriptionPostProcessor
from .enhanced_speaker_diarization import EnhancedSpeakerDiarizer

class LargeFileUltraPrecision:
    """å¤§è¦æ¨¡ãƒ•ã‚¡ã‚¤ãƒ«ç”¨æœ€é«˜å“è³ªãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ï¼ˆå®Ÿç”¨çš„å‡¦ç†æ™‚é–“ï¼‰"""
    
    def __init__(self, chunk_minutes: float = 8.0):
        """
        Initialize large file ultra precision processor
        
        Args:
            chunk_minutes: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºï¼ˆåˆ†ï¼‰
        """
        self.chunk_size = chunk_minutes * 60.0
        self.overlap_seconds = 15.0
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
        self.post_processor = TranscriptionPostProcessor()
        self.speaker_diarizer = EnhancedSpeakerDiarizer(method="acoustic")
        
        # ãƒ­ã‚°è¨­å®š
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
        self.logger = logging.getLogger(__name__)
        
        self.transcriber = None
    
    def _load_transcriber(self, model_size: str, language: str):
        """é«˜å“è³ªTranscriberèª­ã¿è¾¼ã¿"""
        if self.transcriber is None:
            self.logger.info(f"Loading high-quality transcriber: {model_size}")
            self.transcriber = Transcriber(model_size=model_size, language=language)
            self.logger.info("High-quality transcriber loaded")
    
    def _create_chunks(self, audio_file: str) -> List[Dict[str, float]]:
        """éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²"""
        duration = librosa.get_duration(path=audio_file)
        chunks = []
        
        start_time = 0.0
        chunk_id = 0
        
        while start_time < duration:
            end_time = min(start_time + self.chunk_size, duration)
            # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—è¿½åŠ 
            if end_time < duration:
                actual_end_time = min(end_time + self.overlap_seconds, duration)
            else:
                actual_end_time = end_time
            
            chunks.append({
                'id': chunk_id,
                'start': start_time,
                'end': actual_end_time,
                'content_end': end_time,  # å®Ÿéš›ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„çµ‚äº†æ™‚é–“
                'duration': actual_end_time - start_time
            })
            
            chunk_id += 1
            start_time = end_time
        
        self.logger.info(f"Created {len(chunks)} chunks from {duration:.1f}s audio")
        return chunks
    
    def _process_chunk(self, audio_file: str, chunk: Dict[str, float], 
                      settings: Dict[str, Any]) -> List[Dict[str, Any]]:
        """å˜ä¸€ãƒãƒ£ãƒ³ã‚¯ã®æœ€é«˜å“è³ªå‡¦ç†"""
        try:
            self.logger.info(f"Processing chunk {chunk['id']} ({chunk['start']/60:.1f}-{chunk['end']/60:.1f}min)")
            
            # éŸ³å£°èª­ã¿è¾¼ã¿ï¼ˆé«˜å“è³ªè¨­å®šï¼‰
            audio_data, sample_rate = librosa.load(
                audio_file,
                sr=16000,
                offset=chunk['start'],
                duration=chunk['duration'],
                mono=True
            )
            
            # é«˜å“è³ªè»¢å†™
            segment_results = self.transcriber.transcribe_audio(audio_data, sample_rate)
            
            # å½¢å¼å¤‰æ›
            segments = []
            for seg in segment_results:
                seg_data = {
                    'start': seg.start_time + chunk['start'],
                    'end': seg.end_time + chunk['start'],
                    'text': seg.text.strip(),
                    'confidence': seg.confidence
                }
                
                # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—éƒ¨åˆ†ã‚’ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç¯„å›²å†…ã®ã¿ï¼‰
                if seg_data['start'] < chunk['content_end'] and seg_data['text']:
                    # çµ‚äº†æ™‚é–“ã‚‚ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç¯„å›²å†…ã«èª¿æ•´
                    if seg_data['end'] > chunk['content_end']:
                        seg_data['end'] = chunk['content_end']
                    segments.append(seg_data)
            
            # è©±è€…èªè­˜ï¼ˆé«˜ç²¾åº¦ï¼‰
            if settings.get('enable_speaker_recognition', True) and len(segments) > 0:
                try:
                    self.logger.info(f"Applying speaker recognition to chunk {chunk['id']}")
                    speaker_segments = self.speaker_diarizer.diarize_audio(
                        audio_data, sample_rate,
                        num_speakers=settings.get('num_speakers')
                    )
                    
                    # è©±è€…æƒ…å ±ãƒãƒ¼ã‚¸
                    for seg in segments:
                        seg_start_local = seg['start'] - chunk['start']
                        seg_end_local = seg['end'] - chunk['start']
                        
                        best_speaker = 'SPEAKER_UNKNOWN'
                        max_overlap = 0.0
                        
                        for spk_seg in speaker_segments:
                            overlap_start = max(seg_start_local, spk_seg['start'])
                            overlap_end = min(seg_end_local, spk_seg['end'])
                            overlap = max(0, overlap_end - overlap_start)
                            
                            if overlap > max_overlap:
                                max_overlap = overlap
                                best_speaker = spk_seg.get('speaker', 'SPEAKER_UNKNOWN')
                        
                        seg['speaker'] = best_speaker
                        
                except Exception as e:
                    self.logger.warning(f"Speaker recognition failed for chunk {chunk['id']}: {e}")
                    for seg in segments:
                        seg['speaker'] = 'SPEAKER_UNKNOWN'
            
            self.logger.info(f"Chunk {chunk['id']} completed: {len(segments)} segments")
            return segments
            
        except Exception as e:
            self.logger.error(f"Chunk {chunk['id']} processing failed: {e}")
            return []
    
    def _merge_chunks(self, all_chunks_results: List[List[Dict]], chunks: List[Dict]) -> List[Dict]:
        """ãƒãƒ£ãƒ³ã‚¯çµæœã‚’ãƒãƒ¼ã‚¸ï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—å‡¦ç†ï¼‰"""
        if not all_chunks_results:
            return []
        
        merged_segments = []
        
        for i, chunk_segments in enumerate(all_chunks_results):
            if i == 0:
                # æœ€åˆã®ãƒãƒ£ãƒ³ã‚¯ã¯ãã®ã¾ã¾è¿½åŠ 
                merged_segments.extend(chunk_segments)
            else:
                # å¾Œç¶šãƒãƒ£ãƒ³ã‚¯ã¯ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚’é™¤å»
                chunk_start = chunks[i]['start']
                prev_chunk_content_end = chunks[i-1]['content_end']
                
                for segment in chunk_segments:
                    # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ç¯„å›²å¤–ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ã¿è¿½åŠ 
                    if segment['start'] >= prev_chunk_content_end:
                        merged_segments.append(segment)
        
        return merged_segments
    
    def process_file(self, audio_file: str, output_file: str,
                    model: str = 'large-v3',
                    language: str = 'ja',
                    enable_speaker_recognition: bool = True,
                    num_speakers: Optional[int] = None) -> Dict[str, Any]:
        """
        å¤§è¦æ¨¡éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€é«˜å“è³ªå‡¦ç†
        
        Args:
            audio_file: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            output_file: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            model: Whisperãƒ¢ãƒ‡ãƒ« (æ¨å¥¨: 'large-v3')
            language: è¨€èªã‚³ãƒ¼ãƒ‰
            enable_speaker_recognition: è©±è€…èªè­˜ã‚’æœ‰åŠ¹åŒ–
            num_speakers: æœŸå¾…è©±è€…æ•°
            
        Returns:
            å‡¦ç†çµæœ
        """
        start_time = time.time()
        
        if not os.path.exists(audio_file):
            raise FileNotFoundError(f"Audio file not found: {audio_file}")
        
        # Transcriberèª­ã¿è¾¼ã¿
        self._load_transcriber(model, language)
        
        # è¨­å®š
        settings = {
            'language': language,
            'enable_speaker_recognition': enable_speaker_recognition,
            'num_speakers': num_speakers
        }
        
        # ãƒãƒ£ãƒ³ã‚¯ä½œæˆ
        chunks = self._create_chunks(audio_file)
        total_chunks = len(chunks)
        
        # å…¨ãƒãƒ£ãƒ³ã‚¯å‡¦ç†
        all_chunks_results = []
        
        self.logger.info(f"Processing {total_chunks} chunks with ultra precision...")
        
        for i, chunk in enumerate(chunks):
            chunk_start = time.time()
            
            # ãƒãƒ£ãƒ³ã‚¯å‡¦ç†
            segments = self._process_chunk(audio_file, chunk, settings)
            all_chunks_results.append(segments)
            
            chunk_time = time.time() - chunk_start
            progress = (i + 1) / total_chunks * 100
            
            self.logger.info(
                f"Chunk {i+1}/{total_chunks} ({progress:.1f}%): "
                f"{len(segments)} segments, {chunk_time:.1f}s"
            )
        
        # ãƒãƒ£ãƒ³ã‚¯ãƒãƒ¼ã‚¸
        self.logger.info("Merging chunks...")
        all_segments = self._merge_chunks(all_chunks_results, chunks)
        
        # æ—¥æœ¬èªå¾Œå‡¦ç†
        if language == 'ja':
            self.logger.info("Applying Japanese post-processing...")
            try:
                all_segments = self.post_processor.process_transcription(all_segments)
            except Exception as e:
                self.logger.warning(f"Post-processing failed: {e}")
        
        # è©±è€…ä¸€è²«æ€§å‡¦ç†
        if enable_speaker_recognition:
            self.logger.info("Applying speaker consistency...")
            all_segments = self._apply_speaker_consistency(all_segments)
        
        # çµ±è¨ˆè¨ˆç®—
        total_time = time.time() - start_time
        duration = librosa.get_duration(path=audio_file)
        avg_confidence = np.mean([seg.get('confidence', 0.0) for seg in all_segments]) if all_segments else 0.0
        
        # çµæœä½œæˆ
        result = {
            'format_version': '2.0',
            'segments': all_segments,
            'metadata': {
                'original_file': audio_file,
                'total_duration': duration,
                'total_segments': len(all_segments),
                'total_chunks': total_chunks,
                'processing_time': total_time,
                'real_time_factor': duration / total_time if total_time > 0 else 0,
                'average_confidence': avg_confidence,
                'model_used': model,
                'language': language,
                'processing_method': 'large_file_ultra_precision',
                'speaker_recognition_enabled': enable_speaker_recognition
            }
        }
        
        # å‡ºåŠ›ä¿å­˜
        self._save_results(result, output_file)
        
        self.logger.info(
            f"Ultra precision processing completed: {len(all_segments)} segments, "
            f"avg confidence: {avg_confidence:.3f}, "
            f"speed: {duration/total_time:.1f}x realtime, "
            f"total time: {total_time/60:.1f}min"
        )
        
        return result
    
    def _apply_speaker_consistency(self, segments: List[Dict]) -> List[Dict]:
        """è©±è€…ä¸€è²«æ€§ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ é©ç”¨"""
        if len(segments) < 2:
            return segments
        
        # çŸ­ã„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ãƒãƒ¼ã‚¸
        i = 1
        while i < len(segments):
            current = segments[i]
            previous = segments[i-1]
            
            # éå¸¸ã«çŸ­ã„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆï¼ˆ2ç§’æœªæº€ï¼‰ã§åŒã˜è©±è€…ã®å ´åˆã€ãƒãƒ¼ã‚¸
            if (current['end'] - current['start'] < 2.0 and 
                current.get('speaker') == previous.get('speaker') and
                current['start'] - previous['end'] < 1.0):
                
                # ãƒãƒ¼ã‚¸
                previous['end'] = current['end']
                previous['text'] += ' ' + current['text']
                previous['confidence'] = (previous['confidence'] + current['confidence']) / 2
                
                segments.pop(i)
            else:
                i += 1
        
        return segments
    
    def _save_results(self, result: Dict[str, Any], output_file: str):
        """çµæœã®ä¿å­˜"""
        base_path = Path(output_file).with_suffix('')
        
        # JSONå‡ºåŠ›
        json_file = f"{base_path}_ultra.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        
        # CSVå‡ºåŠ›
        csv_file = f"{base_path}_ultra.csv"
        with open(csv_file, 'w', encoding='utf-8') as f:
            f.write("segment_id,start_seconds,end_seconds,duration,text,confidence,speaker\n")
            
            for i, segment in enumerate(result['segments']):
                duration = segment['end'] - segment['start']
                text = segment['text'].replace('"', '""')
                speaker = segment.get('speaker', 'SPEAKER_UNKNOWN')
                confidence = segment.get('confidence', 0.0)
                
                f.write(f"{i},{segment['start']:.3f},{segment['end']:.3f},"
                       f"{duration:.3f},\"{text}\",{confidence:.3f},{speaker}\n")
        
        # SRTå‡ºåŠ›
        srt_file = f"{base_path}_ultra.srt"
        with open(srt_file, 'w', encoding='utf-8') as f:
            for i, segment in enumerate(result['segments']):
                start_time = self._seconds_to_srt_time(segment['start'])
                end_time = self._seconds_to_srt_time(segment['end'])
                
                f.write(f"{i+1}\n")
                f.write(f"{start_time} --> {end_time}\n")
                f.write(f"{segment['text']}\n\n")
        
        self.logger.info(f"Results saved: {json_file}, {csv_file}, {srt_file}")
    
    def _seconds_to_srt_time(self, seconds: float) -> str:
        """ç§’ã‚’SRTæ™‚é–“å½¢å¼ã«å¤‰æ›"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = seconds % 60
        return f"{hours:02d}:{minutes:02d}:{secs:06.3f}".replace('.', ',')

def main():
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Large File Ultra Precision Processor - å¤§è¦æ¨¡éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«æœ€é«˜å“è³ªå‡¦ç†",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  # 2æ™‚é–“éŸ³å£°ã®æœ€é«˜å“è³ªå‡¦ç†
  python -m transcription.large_file_ultra_precision input.mp3 -o output
  
  # ã‚«ã‚¹ã‚¿ãƒ è¨­å®š
  python -m transcription.large_file_ultra_precision input.mp3 -o output --model large-v3 --chunk-size 10
        """
    )
    
    parser.add_argument('audio_file', help='éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    parser.add_argument('-o', '--output', required=True, help='å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹')
    parser.add_argument('--model', default='large-v3',
                       choices=['tiny', 'base', 'small', 'medium', 'large', 'large-v3'],
                       help='Whisperãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º')
    parser.add_argument('--language', default='ja', help='è¨€èªã‚³ãƒ¼ãƒ‰')
    parser.add_argument('--chunk-size', type=float, default=8.0,
                       help='ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºï¼ˆåˆ†ï¼‰')
    parser.add_argument('--num-speakers', type=int, help='æœŸå¾…è©±è€…æ•°')
    parser.add_argument('--no-speaker', action='store_true',
                       help='è©±è€…èªè­˜ã‚’ç„¡åŠ¹åŒ–')
    
    args = parser.parse_args()
    
    # ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼åˆæœŸåŒ–
    processor = LargeFileUltraPrecision(chunk_minutes=args.chunk_size)
    
    try:
        # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
        result = processor.process_file(
            audio_file=args.audio_file,
            output_file=args.output,
            model=args.model,
            language=args.language,
            enable_speaker_recognition=not args.no_speaker,
            num_speakers=args.num_speakers
        )
        
        print(f"\nâœ… æœ€é«˜å“è³ªå‡¦ç†å®Œäº†!")
        print(f"ğŸ“Š çµæœ:")
        print(f"   - ç·ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°: {result['metadata']['total_segments']}")
        print(f"   - å¹³å‡ä¿¡é ¼åº¦: {result['metadata']['average_confidence']:.1%}")
        print(f"   - å‡¦ç†é€Ÿåº¦: {result['metadata']['real_time_factor']:.1f}x ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ")
        print(f"   - å‡¦ç†æ™‚é–“: {result['metadata']['processing_time']/60:.1f}åˆ†")
        print(f"   - éŸ³å£°æ™‚é–“: {result['metadata']['total_duration']/60:.1f}åˆ†")
        print(f"   - ãƒ¢ãƒ‡ãƒ«: {result['metadata']['model_used']}")
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸  ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹ä¸­æ–­")
        sys.exit(130)
    except Exception as e:
        print(f"\nâŒ å‡¦ç†å¤±æ•—: {e}")
        sys.exit(1)

if __name__ == '__main__':
    main()