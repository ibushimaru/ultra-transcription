"""
Main CLI application for audio transcription.
"""

import click
import os
import time
from pathlib import Path
from typing import Optional

from .audio_processor import AudioProcessor
from .transcriber import Transcriber
from .speaker_diarization import SpeakerDiarizer
from .output_formatter import OutputFormatter


@click.command()
@click.argument('audio_file', type=click.Path(exists=True))
@click.option('--output', '-o', help='Output file base path (without extension)')
@click.option('--model', '-m', default='base', 
              type=click.Choice(['tiny', 'base', 'small', 'medium', 'large']),
              help='Whisper model size')
@click.option('--language', '-l', default='ja', help='Language code for transcription')
@click.option('--min-confidence', default=0.3, type=float,
              help='Minimum confidence threshold for segments')
@click.option('--no-noise-reduction', is_flag=True, 
              help='Skip noise reduction preprocessing')
@click.option('--no-speaker-diarization', is_flag=True,
              help='Skip speaker diarization')
@click.option('--no-filter-fillers', is_flag=True,
              help='Skip filler word filtering')
@click.option('--hf-token', help='Hugging Face token for pyannote models')
@click.option('--format', 'output_format', 
              type=click.Choice(['all', 'json', 'csv', 'txt', 'srt']),
              default='all', help='Output format')
def transcribe(audio_file: str, output: Optional[str], model: str, language: str,
               min_confidence: float, no_noise_reduction: bool, 
               no_speaker_diarization: bool, no_filter_fillers: bool,
               hf_token: Optional[str], output_format: str):
    """
    Transcribe audio file to text with speaker identification.
    
    AUDIO_FILE: Path to the audio file (MP3, WAV, etc.)
    """
    
    print("ğŸµ éŸ³å£°æ–‡å­—èµ·ã“ã—ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³")
    print("=" * 50)
    
    # Validate input file
    if not os.path.exists(audio_file):
        click.echo(f"Error: Audio file not found: {audio_file}", err=True)
        return
    
    # Set default output path if not provided
    if not output:
        audio_path = Path(audio_file)
        output = str(audio_path.parent / audio_path.stem)
    
    try:
        start_time = time.time()
        
        # Initialize components
        print("ğŸ”§ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’åˆæœŸåŒ–ä¸­...")
        audio_processor = AudioProcessor()
        transcriber = Transcriber(model_size=model, language=language)
        
        if not no_speaker_diarization:
            speaker_diarizer = SpeakerDiarizer(use_auth_token=hf_token)
        else:
            speaker_diarizer = None
        
        output_formatter = OutputFormatter()
        
        # Process audio
        print(f"ğŸ§ éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ä¸­: {audio_file}")
        audio_data, sample_rate = audio_processor.preprocess_audio(
            audio_file, 
            reduce_noise=not no_noise_reduction
        )
        
        # Get audio duration
        duration = audio_processor.get_audio_duration(audio_file)
        print(f"â±ï¸  éŸ³å£°é•·: {output_formatter.format_timestamp(duration)}")
        
        # Transcribe audio
        print(f"ğŸ“ æ–‡å­—èµ·ã“ã—ä¸­ (ãƒ¢ãƒ‡ãƒ«: {model})...")
        transcription_segments = transcriber.process_transcription(
            audio_data, 
            sample_rate,
            filter_confidence=True,
            filter_fillers=not no_filter_fillers,
            min_confidence=min_confidence
        )
        
        if not transcription_segments:
            print("âš ï¸  æ–‡å­—èµ·ã“ã—çµæœãŒå¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
            print("   - éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®å“è³ªã‚’ç¢ºèªã—ã¦ãã ã•ã„")
            print("   - --min-confidence ã®å€¤ã‚’ä¸‹ã’ã¦ã¿ã¦ãã ã•ã„")
            return
        
        print(f"âœ… {len(transcription_segments)} ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®æ–‡å­—èµ·ã“ã—ãŒå®Œäº†")
        
        # Speaker diarization
        if speaker_diarizer and speaker_diarizer.is_available():
            print("ğŸ‘¥ ã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼è­˜åˆ¥ä¸­...")
            speaker_segments = speaker_diarizer.diarize_audio(audio_data, sample_rate)
            
            # Combine transcription and speaker information
            final_segments = speaker_diarizer.assign_speakers_to_transcription(
                transcription_segments, speaker_segments
            )
            
            # Count unique speakers
            unique_speakers = set(seg["speaker_id"] for seg in final_segments)
            print(f"âœ… {len(unique_speakers)} äººã®ã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼ã‚’æ¤œå‡º")
        else:
            if speaker_diarizer is None:
                print("â­ï¸  ã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼è­˜åˆ¥ã‚’ã‚¹ã‚­ãƒƒãƒ—")
            else:
                print("âš ï¸  ã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼è­˜åˆ¥ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            
            # Convert transcription segments to final format
            final_segments = [
                {
                    "start_time": seg.start_time,
                    "end_time": seg.end_time,
                    "text": seg.text,
                    "confidence": seg.confidence,
                    "speaker_id": "SPEAKER_UNKNOWN"
                }
                for seg in transcription_segments
            ]
        
        # Prepare metadata
        metadata = {
            "input_file": audio_file,
            "model_size": model,
            "language": language,
            "min_confidence": min_confidence,
            "noise_reduction": not no_noise_reduction,
            "speaker_diarization": not no_speaker_diarization,
            "filler_filtering": not no_filter_fillers,
            "audio_duration_seconds": duration,
            "total_segments": len(final_segments),
            "processing_time_seconds": round(time.time() - start_time, 2)
        }
        
        # Save output
        print(f"ğŸ’¾ çµæœã‚’ä¿å­˜ä¸­: {output}")
        
        if output_format == 'all':
            saved_files = output_formatter.save_all_formats(final_segments, output, metadata)
            print("ğŸ“ ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:")
            for format_name, file_path in saved_files.items():
                print(f"   - {format_name.upper()}: {file_path}")
        else:
            output_data = output_formatter.prepare_output_data(final_segments)
            
            if output_format == 'json':
                output_formatter.save_as_json(output_data, f"{output}.json", metadata)
                print(f"   - JSON: {output}.json")
            elif output_format == 'csv':
                output_formatter.save_as_csv(output_data, f"{output}.csv")
                print(f"   - CSV: {output}.csv")
            elif output_format == 'txt':
                output_formatter.save_as_txt(output_data, f"{output}.txt")
                print(f"   - TXT: {output}.txt")
            elif output_format == 'srt':
                output_formatter.save_as_srt(output_data, f"{output}.srt")
                print(f"   - SRT: {output}.srt")
        
        # Display summary
        output_formatter.print_summary(final_segments, duration)
        
        total_time = time.time() - start_time
        print(f"\\nâ±ï¸  ç·å‡¦ç†æ™‚é–“: {total_time:.1f}ç§’")
        print("ğŸ‰ æ–‡å­—èµ·ã“ã—ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        raise


def cli():
    """Audio transcription CLI application."""
    transcribe()


if __name__ == '__main__':
    cli()