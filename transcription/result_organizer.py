#!/usr/bin/env python3
"""
Result Organizer - ãƒ†ã‚¹ãƒˆçµæœæ•´ç†ã‚·ã‚¹ãƒ†ãƒ 

ãƒ†ã‚¹ãƒˆçµæœã‚’ä½“ç³»çš„ã«æ•´ç†ã—ã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ç®¡ç†ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ :
- ãƒ¢ãƒ‡ãƒ«åˆ¥ãƒ•ã‚©ãƒ«ãƒ€åˆ†é¡
- ãƒ†ã‚¹ãƒˆç¨®åˆ¥åˆ†é¡  
- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è‡ªå‹•ç”Ÿæˆ
- æ¯”è¼ƒåˆ†æã‚µãƒãƒ¼ãƒˆ
"""

import os
import json
import shutil
import logging
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime

class ResultOrganizer:
    """ãƒ†ã‚¹ãƒˆçµæœæ•´ç†ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, base_dir: str = "test_outputs"):
        """
        Initialize result organizer
        
        Args:
            base_dir: ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        self.base_dir = Path(base_dir)
        self.organized_dir = self.base_dir / "organized"
        
        # ãƒ­ã‚°è¨­å®š
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
        self.logger = logging.getLogger(__name__)
        
        # æ–°ãƒ•ã‚©ãƒ«ãƒ€æ§‹é€ 
        self.structure = {
            "benchmarks": {
                "short_tests": "30ç§’ä»¥ä¸‹ã®ãƒ†ã‚¹ãƒˆ",
                "medium_tests": "1-5åˆ†ã®ãƒ†ã‚¹ãƒˆ", 
                "long_tests": "å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«ãƒ†ã‚¹ãƒˆ"
            },
            "models": {
                "tiny": "tiny ãƒ¢ãƒ‡ãƒ«çµæœ",
                "small": "small ãƒ¢ãƒ‡ãƒ«çµæœ",
                "medium": "medium ãƒ¢ãƒ‡ãƒ«çµæœ", 
                "large": "large ãƒ¢ãƒ‡ãƒ«çµæœ",
                "large-v3-turbo": "large-v3-turbo ãƒ¢ãƒ‡ãƒ«çµæœ",
                "turbo": "turbo ãƒ¢ãƒ‡ãƒ«çµæœ"
            },
            "systems": {
                "rapid_ultra": "rapid_ultra_processorçµæœ",
                "large_file_ultra": "large_file_ultra_precisionçµæœ",
                "segmented": "segmented_processorçµæœ",
                "ultra_precision": "ultra_precision_speakerçµæœ"
            },
            "reference": {
                "quality_baseline": "å“è³ªå‚ç…§ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³",
                "performance_baseline": "æ€§èƒ½å‚ç…§ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³"
            },
            "production": {
                "final_results": "æœ¬ç•ªä½¿ç”¨å¯èƒ½ãªæœ€çµ‚çµæœ"
            }
        }
    
    def create_organized_structure(self):
        """æ–°ã—ã„ãƒ•ã‚©ãƒ«ãƒ€æ§‹é€ ã‚’ä½œæˆ"""
        self.logger.info("Creating organized directory structure...")
        
        for category, subcategories in self.structure.items():
            category_path = self.organized_dir / category
            category_path.mkdir(parents=True, exist_ok=True)
            
            if isinstance(subcategories, dict):
                for subcat, description in subcategories.items():
                    subcat_path = category_path / subcat
                    subcat_path.mkdir(exist_ok=True)
                    
                    # READMEãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
                    readme_path = subcat_path / "README.md"
                    with open(readme_path, 'w', encoding='utf-8') as f:
                        f.write(f"# {subcat}\n\n{description}\n\n")
                        f.write(f"ä½œæˆæ—¥: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    def extract_metadata_from_json(self, json_file: Path) -> Dict[str, Any]:
        """JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            metadata = {
                "file_name": json_file.name,
                "file_size_mb": json_file.stat().st_size / (1024 * 1024),
                "created_date": datetime.fromtimestamp(json_file.stat().st_mtime).isoformat(),
                "segments_count": len(data.get('segments', [])),
                "format_version": data.get('format_version', 'unknown')
            }
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ section ãŒã‚ã‚Œã°è¿½åŠ 
            if 'metadata' in data:
                meta = data['metadata']
                metadata.update({
                    "model_used": meta.get('model_used', 'unknown'),
                    "language": meta.get('language', 'unknown'),
                    "processing_method": meta.get('processing_method', 'unknown'),
                    "total_duration": meta.get('total_duration', 0),
                    "average_confidence": meta.get('average_confidence', 0),
                    "processing_time": meta.get('processing_time', 0),
                    "real_time_factor": meta.get('real_time_factor', 0)
                })
            
            # summary section ãŒã‚ã‚Œã°è¿½åŠ 
            if 'summary' in data:
                summary = data['summary']
                metadata.update({
                    "speaker_count": summary.get('speaker_count', 0),
                    "total_duration": summary.get('total_duration_seconds', 0)
                })
            
            return metadata
            
        except Exception as e:
            self.logger.error(f"Failed to extract metadata from {json_file}: {e}")
            return {
                "file_name": json_file.name,
                "error": str(e),
                "file_size_mb": json_file.stat().st_size / (1024 * 1024),
                "created_date": datetime.fromtimestamp(json_file.stat().st_mtime).isoformat()
            }
    
    def classify_file(self, file_path: Path) -> Dict[str, str]:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†é¡"""
        file_name = file_path.name.lower()
        
        # ãƒ¢ãƒ‡ãƒ«åˆ†é¡
        model = "unknown"
        if any(m in file_name for m in ["large-v3-turbo", "large_v3_turbo"]):
            model = "large-v3-turbo"
        elif "turbo" in file_name:
            model = "turbo"
        elif "large" in file_name:
            model = "large"
        elif "medium" in file_name:
            model = "medium"
        elif "small" in file_name:
            model = "small"
        elif "tiny" in file_name:
            model = "tiny"
        
        # ã‚·ã‚¹ãƒ†ãƒ åˆ†é¡
        system = "unknown"
        if "segmented" in file_name:
            system = "segmented"
        elif "rapid_ultra" in file_name or "rapid" in file_name:
            system = "rapid_ultra"
        elif "large_file" in file_name:
            system = "large_file_ultra"
        elif "ultra_precision" in file_name:
            system = "ultra_precision"
        
        # ãƒ†ã‚¹ãƒˆã‚µã‚¤ã‚ºåˆ†é¡
        test_size = "unknown"
        if "ultra_short" in file_name or "30s" in file_name:
            test_size = "short"
        elif "90s" in file_name or "test_90" in file_name:
            test_size = "medium"
        elif "large_file" in file_name or "segmented" in file_name:
            test_size = "long"
        
        # å“è³ªåˆ†é¡
        quality_type = "normal"
        if "final" in file_name or "optimized" in file_name:
            quality_type = "reference"
        elif "precision" in file_name:
            quality_type = "high_quality"
        
        return {
            "model": model,
            "system": system,
            "test_size": test_size,
            "quality_type": quality_type
        }
    
    def organize_files(self):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ•´ç†"""
        self.logger.info("Starting file organization...")
        
        # æ–°æ§‹é€ ä½œæˆ
        self.create_organized_structure()
        
        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
        json_files = list(self.base_dir.glob("*.json"))
        
        organized_files = []
        metadata_collection = []
        
        for json_file in json_files:
            # ãƒ•ã‚¡ã‚¤ãƒ«åˆ†é¡
            classification = self.classify_file(json_file)
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
            metadata = self.extract_metadata_from_json(json_file)
            metadata.update(classification)
            metadata_collection.append(metadata)
            
            # å¯¾å¿œã™ã‚‹ä»–ã®ãƒ•ã‚¡ã‚¤ãƒ« (.csv, .srt) ã‚‚å–å¾—
            base_name = json_file.stem
            related_files = []
            
            for ext in ['.json', '.csv', '.srt']:
                related_file = json_file.with_suffix(ext)
                if related_file.exists():
                    related_files.append(related_file)
            
            # ä¿å­˜å…ˆæ±ºå®š
            target_dir = self._determine_target_directory(classification)
            target_dir.mkdir(parents=True, exist_ok=True)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ç§»å‹•
            for file_path in related_files:
                target_file = target_dir / file_path.name
                if not target_file.exists():
                    shutil.copy2(file_path, target_file)
                    self.logger.info(f"Moved: {file_path.name} -> {target_dir}")
            
            organized_files.append({
                "original_path": str(json_file),
                "target_directory": str(target_dir),
                "classification": classification,
                "related_files": [f.name for f in related_files]
            })
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼ä½œæˆ
        self._create_metadata_summary(metadata_collection, organized_files)
        
        self.logger.info(f"Organization completed: {len(organized_files)} file sets processed")
        return organized_files
    
    def _determine_target_directory(self, classification: Dict[str, str]) -> Path:
        """åˆ†é¡ã«åŸºã¥ã„ã¦ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ±ºå®š"""
        model = classification['model']
        system = classification['system']
        test_size = classification['test_size']
        quality_type = classification['quality_type']
        
        # å‚ç…§å“è³ªã®å ´åˆ
        if quality_type == "reference":
            return self.organized_dir / "reference" / "quality_baseline"
        
        # ã‚·ã‚¹ãƒ†ãƒ åˆ¥åˆ†é¡ã‚’å„ªå…ˆ
        if system != "unknown":
            system_dir = self.organized_dir / "systems" / system
            if model != "unknown":
                return system_dir / model
            return system_dir
        
        # ãƒ¢ãƒ‡ãƒ«åˆ¥åˆ†é¡
        if model != "unknown":
            model_dir = self.organized_dir / "models" / model
            if test_size == "short":
                return model_dir / "short_tests"
            elif test_size == "medium":
                return model_dir / "medium_tests"
            elif test_size == "long":
                return model_dir / "long_tests"
            return model_dir
        
        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯åˆ†é¡
        if test_size == "short":
            return self.organized_dir / "benchmarks" / "short_tests"
        elif test_size == "medium":
            return self.organized_dir / "benchmarks" / "medium_tests"
        elif test_size == "long":
            return self.organized_dir / "benchmarks" / "long_tests"
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        return self.organized_dir / "uncategorized"
    
    def _create_metadata_summary(self, metadata_collection: List[Dict], organized_files: List[Dict]):
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆ"""
        summary = {
            "organization_date": datetime.now().isoformat(),
            "total_files_processed": len(metadata_collection),
            "statistics": {
                "models": {},
                "systems": {},
                "test_sizes": {},
                "quality_types": {}
            },
            "files": metadata_collection,
            "organization_log": organized_files
        }
        
        # çµ±è¨ˆè¨ˆç®—
        for metadata in metadata_collection:
            for key in ["model", "system", "test_size", "quality_type"]:
                if key in metadata:
                    value = metadata[key]
                    if value not in summary["statistics"][f"{key}s"]:
                        summary["statistics"][f"{key}s"][value] = 0
                    summary["statistics"][f"{key}s"][value] += 1
        
        # ã‚µãƒãƒªãƒ¼ä¿å­˜
        summary_file = self.organized_dir / "metadata_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        # äººé–“ãŒèª­ã¿ã‚„ã™ã„ã‚µãƒãƒªãƒ¼ä½œæˆ
        readme_file = self.organized_dir / "README.md"
        with open(readme_file, 'w', encoding='utf-8') as f:
            f.write("# ãƒ†ã‚¹ãƒˆçµæœæ•´ç†ã‚µãƒãƒªãƒ¼\n\n")
            f.write(f"æ•´ç†æ—¥æ™‚: {summary['organization_date']}\n")
            f.write(f"å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {summary['total_files_processed']}\n\n")
            
            f.write("## çµ±è¨ˆ\n\n")
            for category, stats in summary["statistics"].items():
                f.write(f"### {category.title()}\n")
                for item, count in stats.items():
                    f.write(f"- {item}: {count}å€‹\n")
                f.write("\n")
            
            f.write("## ãƒ•ã‚©ãƒ«ãƒ€æ§‹é€ \n\n")
            for category in self.structure.keys():
                f.write(f"- `{category}/`: {category}é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«\n")
        
        self.logger.info(f"Metadata summary saved: {summary_file}")

def main():
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Result Organizer - ãƒ†ã‚¹ãƒˆçµæœæ•´ç†ã‚·ã‚¹ãƒ†ãƒ "
    )
    
    parser.add_argument('--base-dir', default='test_outputs',
                       help='ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    parser.add_argument('--dry-run', action='store_true',
                       help='å®Ÿéš›ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç§»å‹•ã›ãšã«åˆ†æã®ã¿')
    
    args = parser.parse_args()
    
    organizer = ResultOrganizer(args.base_dir)
    
    if args.dry_run:
        print("DRY RUN: ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æã®ã¿å®Ÿè¡Œ...")
        # TODO: dry run implementation
    else:
        organized_files = organizer.organize_files()
        print(f"\nâœ… æ•´ç†å®Œäº†: {len(organized_files)} ãƒ•ã‚¡ã‚¤ãƒ«ã‚»ãƒƒãƒˆã‚’å‡¦ç†")
        print(f"ğŸ“ æ•´ç†çµæœ: {organizer.organized_dir}")

if __name__ == '__main__':
    main()