#!/usr/bin/env python3
"""
Rapid Ultra Precision Processor - å¤§è¦æ¨¡ãƒ•ã‚¡ã‚¤ãƒ«ç”¨æœ€é«˜å“è³ªå‡¦ç†

2æ™‚é–“ä»¥ä¸Šã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æœ€é«˜å“è³ªã§å‡¦ç†ã™ã‚‹ç‰¹åŒ–ã‚·ã‚¹ãƒ†ãƒ :
- large-v3ãƒ¢ãƒ‡ãƒ«å¯¾å¿œ
- é«˜å“è³ªWhisperè¨­å®š (beam_size=5, best_of=5)
- é«˜ç²¾åº¦è©±è€…èªè­˜
- æ—¥æœ¬èªå¾Œå‡¦ç†æœ€é©åŒ–
- ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã§å¤§è¦æ¨¡ãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œ
- å®Ÿç”¨çš„ãªå‡¦ç†æ™‚é–“ã¨ã®ãƒãƒ©ãƒ³ã‚¹
"""

import os
import sys
import time
import json
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
import tempfile

import librosa
import numpy as np
import whisper

from .post_processor import TranscriptionPostProcessor
from .enhanced_speaker_diarization import EnhancedSpeakerDiarizer

class RapidUltraPrecisionProcessor:
    """2æ™‚é–“ä»¥ä¸Šã®å¤§è¦æ¨¡ãƒ•ã‚¡ã‚¤ãƒ«å°‚ç”¨æœ€é«˜å“è³ªãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼"""
    
    def __init__(self, chunk_minutes: float = 3.0):
        """
        Initialize rapid processor
        
        Args:
            chunk_minutes: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºï¼ˆåˆ†ï¼‰
        """
        self.chunk_size = chunk_minutes * 60.0
        self.overlap_seconds = 10.0  # çŸ­ã„ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—
        
        # è»½é‡ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
        self.post_processor = TranscriptionPostProcessor()
        self.speaker_diarizer = EnhancedSpeakerDiarizer(method="acoustic")
        
        # ãƒ­ã‚°è¨­å®š
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
        self.logger = logging.getLogger(__name__)
        
        self.whisper_model = None
    
    def _load_model(self, model_size: str):
        """Whisperãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿"""
        if self.whisper_model is None:
            self.logger.info(f"Loading Whisper model: {model_size}")
            self.whisper_model = whisper.load_model(model_size)
            self.logger.info("Model loaded successfully")
    
    def _create_chunks(self, audio_file: str) -> List[Dict[str, float]]:
        """éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²"""
        duration = librosa.get_duration(path=audio_file)
        chunks = []
        
        start_time = 0.0
        chunk_id = 0
        
        while start_time < duration:
            end_time = min(start_time + self.chunk_size, duration)
            
            chunks.append({
                'id': chunk_id,
                'start': start_time,
                'end': end_time,
                'duration': end_time - start_time
            })
            
            chunk_id += 1
            start_time = end_time
        
        self.logger.info(f"Created {len(chunks)} chunks from {duration:.1f}s audio")
        return chunks
    
    def _process_chunk(self, audio_file: str, chunk: Dict[str, float], 
                      settings: Dict[str, Any]) -> List[Dict[str, Any]]:
        """å˜ä¸€ãƒãƒ£ãƒ³ã‚¯ã®é«˜å“è³ªå‡¦ç†"""
        try:
            # éŸ³å£°èª­ã¿è¾¼ã¿ï¼ˆé«˜å“è³ªè¨­å®šï¼‰
            audio_data, sample_rate = librosa.load(
                audio_file,
                sr=16000,  # å›ºå®šã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆ
                offset=chunk['start'],
                duration=chunk['duration'],
                mono=True
            )
            
            # Whisperè»¢å†™ï¼ˆãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸé«˜å“è³ªè¨­å®šï¼‰
            result = self.whisper_model.transcribe(
                audio_data,
                language=settings.get('language', 'ja'),
                task='transcribe',
                verbose=False,
                word_timestamps=False,
                temperature=0.0,  # æ±ºå®šè«–çš„
                beam_size=3,      # é©åº¦ãªãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒ
                best_of=2,        # è»½é‡ãªå€™è£œé¸æŠ
                condition_on_previous_text=True,  # æ–‡è„ˆè€ƒæ…®
                fp16=True         # é«˜é€ŸåŒ–
            )
            
            segments = []
            for segment in result['segments']:
                seg_data = {
                    'start': segment['start'] + chunk['start'],
                    'end': segment['end'] + chunk['start'],
                    'text': segment['text'].strip(),
                    'confidence': 1.0 - segment.get('no_speech_prob', 0.0)
                }
                
                if seg_data['text']:  # ç©ºã§ãªã„ãƒ†ã‚­ã‚¹ãƒˆã®ã¿
                    segments.append(seg_data)
            
            # è©±è€…èªè­˜ï¼ˆé«˜å“è³ªï¼‰
            if settings.get('enable_speaker_recognition', True) and len(segments) > 0:
                try:
                    speaker_segments = self.speaker_diarizer.diarize_audio(
                        audio_data, sample_rate,
                        num_speakers=settings.get('num_speakers')
                    )
                    
                    # è©±è€…æƒ…å ±ãƒãƒ¼ã‚¸
                    for seg in segments:
                        seg_start_local = seg['start'] - chunk['start']
                        seg_end_local = seg['end'] - chunk['start']
                        
                        best_speaker = 'SPEAKER_UNKNOWN'
                        max_overlap = 0.0
                        
                        for spk_seg in speaker_segments:
                            overlap_start = max(seg_start_local, spk_seg.start_time)
                            overlap_end = min(seg_end_local, spk_seg.end_time)
                            overlap = max(0, overlap_end - overlap_start)
                            
                            if overlap > max_overlap:
                                max_overlap = overlap
                                best_speaker = spk_seg.speaker_id
                        
                        seg['speaker'] = best_speaker
                        
                except Exception as e:
                    self.logger.warning(f"Speaker recognition failed: {e}")
                    for seg in segments:
                        seg['speaker'] = 'SPEAKER_UNKNOWN'
            
            return segments
            
        except Exception as e:
            self.logger.error(f"Chunk {chunk['id']} processing failed: {e}")
            return []
    
    def process_file(self, audio_file: str, output_file: str,
                    model: str = 'medium',
                    language: str = 'ja',
                    enable_speaker_recognition: bool = True,
                    num_speakers: Optional[int] = None) -> Dict[str, Any]:
        """
        å¤§è¦æ¨¡éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®é«˜é€Ÿå‡¦ç†
        
        Args:
            audio_file: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            output_file: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            model: Whisperãƒ¢ãƒ‡ãƒ« ('tiny', 'base', 'small', 'medium', 'large')
            language: è¨€èªã‚³ãƒ¼ãƒ‰
            enable_speaker_recognition: è©±è€…èªè­˜ã‚’æœ‰åŠ¹åŒ–
            num_speakers: æœŸå¾…è©±è€…æ•°
            
        Returns:
            å‡¦ç†çµæœ
        """
        start_time = time.time()
        
        if not os.path.exists(audio_file):
            raise FileNotFoundError(f"Audio file not found: {audio_file}")
        
        # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        self._load_model(model)
        
        # è¨­å®š
        settings = {
            'language': language,
            'enable_speaker_recognition': enable_speaker_recognition,
            'num_speakers': num_speakers
        }
        
        # ãƒãƒ£ãƒ³ã‚¯ä½œæˆ
        chunks = self._create_chunks(audio_file)
        total_chunks = len(chunks)
        
        # å…¨ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåé›†
        all_segments = []
        
        self.logger.info(f"Processing {total_chunks} chunks...")
        
        for i, chunk in enumerate(chunks):
            chunk_start = time.time()
            
            # ãƒãƒ£ãƒ³ã‚¯å‡¦ç†
            segments = self._process_chunk(audio_file, chunk, settings)
            all_segments.extend(segments)
            
            chunk_time = time.time() - chunk_start
            progress = (i + 1) / total_chunks * 100
            
            self.logger.info(
                f"Chunk {i+1}/{total_chunks} ({progress:.1f}%): "
                f"{len(segments)} segments, {chunk_time:.1f}s"
            )
        
        # æ—¥æœ¬èªå¾Œå‡¦ç†
        if language == 'ja':
            self.logger.info("Applying Japanese post-processing...")
            try:
                # Post-processorãŒæœŸå¾…ã™ã‚‹å½¢å¼ã«å¤‰æ›
                formatted_segments = []
                for seg in all_segments:
                    formatted_seg = {
                        'start_time': seg['start'],
                        'end_time': seg['end'],
                        'text': seg['text'],
                        'confidence': seg['confidence'],
                        'speaker': seg.get('speaker', 'SPEAKER_UNKNOWN')
                    }
                    formatted_segments.append(formatted_seg)
                
                processed_segments = self.post_processor.process_transcription(formatted_segments)
                
                # å…ƒã®å½¢å¼ã«æˆ»ã™
                all_segments = []
                for seg in processed_segments:
                    converted_seg = {
                        'start': seg['start_time'],
                        'end': seg['end_time'],
                        'text': seg['text'],
                        'confidence': seg['confidence'],
                        'speaker': seg.get('speaker', 'SPEAKER_UNKNOWN')
                    }
                    all_segments.append(converted_seg)
                    
            except Exception as e:
                self.logger.warning(f"Post-processing failed: {e}")
        
        # çµ±è¨ˆè¨ˆç®—
        total_time = time.time() - start_time
        duration = librosa.get_duration(path=audio_file)
        avg_confidence = np.mean([seg.get('confidence', 0.0) for seg in all_segments]) if all_segments else 0.0
        
        # çµæœä½œæˆ
        result = {
            'format_version': '2.0',
            'segments': all_segments,
            'metadata': {
                'original_file': audio_file,
                'total_duration': duration,
                'total_segments': len(all_segments),
                'total_chunks': total_chunks,
                'processing_time': total_time,
                'real_time_factor': duration / total_time if total_time > 0 else 0,
                'average_confidence': avg_confidence,
                'model_used': model,
                'language': language,
                'processing_method': 'rapid_ultra_precision'
            }
        }
        
        # å‡ºåŠ›ä¿å­˜
        self._save_results(result, output_file)
        
        self.logger.info(
            f"Processing completed: {len(all_segments)} segments, "
            f"avg confidence: {avg_confidence:.3f}, "
            f"speed: {duration/total_time:.1f}x realtime"
        )
        
        return result
    
    def _save_results(self, result: Dict[str, Any], output_file: str):
        """çµæœã®ä¿å­˜"""
        base_path = Path(output_file).with_suffix('')
        
        # JSONå‡ºåŠ›
        json_file = f"{base_path}_ultra_precision.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        
        # CSVå‡ºåŠ›
        csv_file = f"{base_path}_ultra_precision.csv"
        with open(csv_file, 'w', encoding='utf-8') as f:
            f.write("segment_id,start_seconds,end_seconds,duration,text,confidence,speaker\n")
            
            for i, segment in enumerate(result['segments']):
                duration = segment['end'] - segment['start']
                text = segment['text'].replace('"', '""')  # CSV ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—
                speaker = segment.get('speaker', 'SPEAKER_UNKNOWN')
                confidence = segment.get('confidence', 0.0)
                
                f.write(f"{i},{segment['start']:.3f},{segment['end']:.3f},"
                       f"{duration:.3f},\"{text}\",{confidence:.3f},{speaker}\n")
        
        # SRTå‡ºåŠ›
        srt_file = f"{base_path}_ultra_precision.srt"
        with open(srt_file, 'w', encoding='utf-8') as f:
            for i, segment in enumerate(result['segments']):
                start_time = self._seconds_to_srt_time(segment['start'])
                end_time = self._seconds_to_srt_time(segment['end'])
                
                f.write(f"{i+1}\n")
                f.write(f"{start_time} --> {end_time}\n")
                f.write(f"{segment['text']}\n\n")
        
        self.logger.info(f"Results saved: {json_file}, {csv_file}, {srt_file}")
    
    def _seconds_to_srt_time(self, seconds: float) -> str:
        """ç§’ã‚’SRTæ™‚é–“å½¢å¼ã«å¤‰æ›"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = seconds % 60
        return f"{hours:02d}:{minutes:02d}:{secs:06.3f}".replace('.', ',')

def main():
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Rapid Ultra Precision Processor - å¤§è¦æ¨¡éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«æœ€é«˜å“è³ªå‡¦ç†",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  # 2æ™‚é–“éŸ³å£°ã®æœ€é«˜å“è³ªå‡¦ç†
  python -m transcription.rapid_ultra_processor input.mp3 -o output --model large-v3
  
  # ã‚«ã‚¹ã‚¿ãƒ ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º
  python -m transcription.rapid_ultra_processor input.mp3 -o output --chunk-size 4
  
  # è©±è€…èªè­˜ãªã—ï¼ˆé«˜é€Ÿï¼‰
  python -m transcription.rapid_ultra_processor input.mp3 -o output --no-speaker
        """
    )
    
    parser.add_argument('audio_file', help='éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    parser.add_argument('-o', '--output', required=True, help='å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹')
    parser.add_argument('--model', default='large-v3',
                       choices=['tiny', 'base', 'small', 'medium', 'large', 'large-v3', 'large-v3-turbo', 'turbo'],
                       help='Whisperãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º (turbo: 8å€é«˜é€ŸåŒ–ç‰ˆ)')
    parser.add_argument('--language', default='ja', help='è¨€èªã‚³ãƒ¼ãƒ‰')
    parser.add_argument('--chunk-size', type=float, default=3.0,
                       help='ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºï¼ˆåˆ†ï¼‰')
    parser.add_argument('--num-speakers', type=int, help='æœŸå¾…è©±è€…æ•°')
    parser.add_argument('--no-speaker', action='store_true',
                       help='è©±è€…èªè­˜ã‚’ç„¡åŠ¹åŒ–')
    
    args = parser.parse_args()
    
    # ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼åˆæœŸåŒ–
    processor = RapidUltraPrecisionProcessor(chunk_minutes=args.chunk_size)
    
    try:
        # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
        result = processor.process_file(
            audio_file=args.audio_file,
            output_file=args.output,
            model=args.model,
            language=args.language,
            enable_speaker_recognition=not args.no_speaker,
            num_speakers=args.num_speakers
        )
        
        print(f"\nâœ… æœ€é«˜å“è³ªå‡¦ç†å®Œäº†!")
        print(f"ğŸ“Š çµæœ:")
        print(f"   - ç·ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°: {result['metadata']['total_segments']}")
        print(f"   - å¹³å‡ä¿¡é ¼åº¦: {result['metadata']['average_confidence']:.1%}")
        print(f"   - å‡¦ç†é€Ÿåº¦: {result['metadata']['real_time_factor']:.1f}x ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ")
        print(f"   - å‡¦ç†æ™‚é–“: {result['metadata']['processing_time']:.1f}ç§’")
        print(f"   - éŸ³å£°æ™‚é–“: {result['metadata']['total_duration']:.1f}ç§’")
        print(f"   - ãƒ¢ãƒ‡ãƒ«: {result['metadata']['model_used']}")
        print(f"   - å‡¦ç†æ–¹å¼: {result['metadata']['processing_method']}")
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸  ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹ä¸­æ–­")
        sys.exit(130)
    except Exception as e:
        print(f"\nâŒ å‡¦ç†å¤±æ•—: {e}")
        sys.exit(1)

if __name__ == '__main__':
    main()