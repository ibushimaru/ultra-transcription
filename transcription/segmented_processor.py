#!/usr/bin/env python3
"""
Segmented Large File Processor - å¤§å®¹é‡éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«åˆ†å‰²å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ 

é•·æ™‚é–“éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†å‰²ã—ã¦å€‹åˆ¥å‡¦ç†ã—ã€çµæœã‚’ãƒãƒ¼ã‚¸:
- 10åˆ†ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«è‡ªå‹•åˆ†å‰²
- å„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ç‹¬ç«‹å‡¦ç†
- çµæœã®è‡ªå‹•ãƒãƒ¼ã‚¸
- ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå•é¡Œè§£æ±º
"""

import os
import sys
import time
import json
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
import tempfile

import librosa
import soundfile as sf
import numpy as np

from .large_file_ultra_precision import LargeFileUltraPrecision

class SegmentedProcessor:
    """å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«ç”¨åˆ†å‰²å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, segment_minutes: float = 10.0, overlap_seconds: float = 5.0):
        """
        Initialize segmented processor
        
        Args:
            segment_minutes: ã‚»ã‚°ãƒ¡ãƒ³ãƒˆé•·ï¼ˆåˆ†ï¼‰
            overlap_seconds: ã‚»ã‚°ãƒ¡ãƒ³ãƒˆé–“ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ï¼ˆç§’ï¼‰
        """
        self.segment_length = segment_minutes * 60.0
        self.overlap_length = overlap_seconds
        
        # ãƒ­ã‚°è¨­å®š
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
        self.logger = logging.getLogger(__name__)
        
        self.processor = LargeFileUltraPrecision(chunk_minutes=2.0)  # å°ã•ãªãƒãƒ£ãƒ³ã‚¯
    
    def _create_segments(self, audio_file: str, temp_dir: str) -> List[Dict[str, Any]]:
        """éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«åˆ†å‰²"""
        duration = librosa.get_duration(path=audio_file)
        audio_data, sample_rate = librosa.load(audio_file, sr=None)
        
        segments = []
        start_time = 0.0
        segment_id = 0
        
        while start_time < duration:
            # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆç¯„å›²è¨ˆç®—
            end_time = min(start_time + self.segment_length, duration)
            
            # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—è¿½åŠ ï¼ˆæœ€å¾Œä»¥å¤–ï¼‰
            if end_time < duration:
                actual_end_time = min(end_time + self.overlap_length, duration)
            else:
                actual_end_time = end_time
            
            # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆéŸ³å£°æŠ½å‡º
            start_sample = int(start_time * sample_rate)
            end_sample = int(actual_end_time * sample_rate)
            segment_audio = audio_data[start_sample:end_sample]
            
            # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            segment_file = os.path.join(temp_dir, f"segment_{segment_id:03d}.wav")
            sf.write(segment_file, segment_audio, sample_rate)
            
            segments.append({
                'id': segment_id,
                'file': segment_file,
                'start_time': start_time,
                'end_time': actual_end_time,
                'content_end_time': end_time,  # å®Ÿéš›ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„çµ‚äº†æ™‚é–“
                'duration': actual_end_time - start_time
            })
            
            self.logger.info(f"Segment {segment_id}: {start_time/60:.1f}-{actual_end_time/60:.1f}min ({segment_file})")
            
            segment_id += 1
            start_time = end_time
        
        self.logger.info(f"Created {len(segments)} segments from {duration/60:.1f}min audio")
        return segments
    
    def _process_segment(self, segment: Dict[str, Any], output_dir: str,
                        model: str, language: str, enable_speaker: bool) -> Optional[Dict[str, Any]]:
        """å˜ä¸€ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®å‡¦ç†"""
        try:
            segment_output = os.path.join(output_dir, f"segment_{segment['id']:03d}")
            
            self.logger.info(f"Processing segment {segment['id']} ({segment['duration']/60:.1f}min)...")
            
            # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆå‡¦ç†
            result = self.processor.process_file(
                audio_file=segment['file'],
                output_file=segment_output,
                model=model,
                language=language,
                enable_speaker_recognition=enable_speaker,
                num_speakers=None
            )
            
            # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæƒ…å ±ã‚’çµæœã«è¿½åŠ 
            result['segment_info'] = segment
            
            return result
            
        except Exception as e:
            self.logger.error(f"Segment {segment['id']} processing failed: {e}")
            return None
    
    def _merge_segments(self, segment_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ã‚»ã‚°ãƒ¡ãƒ³ãƒˆçµæœã‚’ãƒãƒ¼ã‚¸"""
        if not segment_results:
            return {'segments': [], 'metadata': {}}
        
        merged_segments = []
        total_duration = 0.0
        total_processing_time = 0.0
        
        for i, result in enumerate(segment_results):
            if not result or 'segments' not in result:
                continue
            
            segment_info = result['segment_info']
            offset = segment_info['start_time']
            content_end = segment_info['content_end_time']
            
            # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆå†…ã®è»¢å†™çµæœã‚’èª¿æ•´
            for seg in result['segments']:
                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—èª¿æ•´
                adjusted_start = seg['start'] + offset
                adjusted_end = seg['end'] + offset
                
                # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ç¯„å›²å¤–ã‚’ãƒ•ã‚£ãƒ«ã‚¿
                if i > 0:  # æœ€åˆã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆä»¥å¤–
                    prev_content_end = segment_results[i-1]['segment_info']['content_end_time']
                    if adjusted_start < prev_content_end:
                        continue  # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—éƒ¨åˆ†ã‚’ã‚¹ã‚­ãƒƒãƒ—
                
                # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç¯„å›²å†…ã®ã¿
                if adjusted_start < content_end:
                    # çµ‚äº†æ™‚é–“ã‚‚ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç¯„å›²å†…ã«èª¿æ•´
                    if adjusted_end > content_end:
                        adjusted_end = content_end
                    
                    merged_seg = {
                        'start': adjusted_start,
                        'end': adjusted_end,
                        'text': seg['text'],
                        'confidence': seg['confidence'],
                        'speaker': seg.get('speaker', 'SPEAKER_UNKNOWN')
                    }
                    merged_segments.append(merged_seg)
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç´¯ç©
            if 'metadata' in result:
                total_duration = max(total_duration, segment_info['content_end_time'])
                total_processing_time += result['metadata'].get('processing_time', 0)
        
        # çµ±è¨ˆè¨ˆç®—
        avg_confidence = np.mean([seg['confidence'] for seg in merged_segments]) if merged_segments else 0.0
        
        return {
            'format_version': '2.0',
            'segments': merged_segments,
            'metadata': {
                'total_duration': total_duration,
                'total_segments': len(merged_segments),
                'total_processing_time': total_processing_time,
                'real_time_factor': total_duration / total_processing_time if total_processing_time > 0 else 0,
                'average_confidence': avg_confidence,
                'processing_method': 'segmented_ultra_precision',
                'segment_count': len(segment_results)
            }
        }
    
    def process_file(self, audio_file: str, output_file: str,
                    model: str = 'large-v3',
                    language: str = 'ja',
                    enable_speaker_recognition: bool = False) -> Dict[str, Any]:
        """
        å¤§å®¹é‡éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ†å‰²å‡¦ç†
        
        Args:
            audio_file: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            output_file: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            model: Whisperãƒ¢ãƒ‡ãƒ«
            language: è¨€èªã‚³ãƒ¼ãƒ‰
            enable_speaker_recognition: è©±è€…èªè­˜ã‚’æœ‰åŠ¹åŒ–
            
        Returns:
            å‡¦ç†çµæœ
        """
        start_time = time.time()
        
        if not os.path.exists(audio_file):
            raise FileNotFoundError(f"Audio file not found: {audio_file}")
        
        # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        with tempfile.TemporaryDirectory() as temp_dir:
            # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†å‰²
            self.logger.info("Splitting audio into segments...")
            segments = self._create_segments(audio_file, temp_dir)
            
            # å„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆå‡¦ç†
            segment_results = []
            output_dir = Path(output_file).parent
            
            for i, segment in enumerate(segments):
                self.logger.info(f"Processing segment {i+1}/{len(segments)}...")
                
                result = self._process_segment(
                    segment, str(output_dir), model, language, enable_speaker_recognition
                )
                
                if result:
                    segment_results.append(result)
                    self.logger.info(f"Segment {i+1} completed: {len(result['segments'])} segments")
                else:
                    self.logger.warning(f"Segment {i+1} failed")
        
        # çµæœãƒãƒ¼ã‚¸
        self.logger.info("Merging segment results...")
        final_result = self._merge_segments(segment_results)
        
        # è¿½åŠ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        total_time = time.time() - start_time
        final_result['metadata'].update({
            'original_file': audio_file,
            'model_used': model,
            'language': language,
            'total_processing_time': total_time,
            'segments_processed': len(segments),
            'segments_successful': len(segment_results)
        })
        
        # çµæœä¿å­˜
        self._save_results(final_result, output_file)
        
        self.logger.info(
            f"Segmented processing completed: {len(final_result['segments'])} segments, "
            f"avg confidence: {final_result['metadata']['average_confidence']:.3f}, "
            f"total time: {total_time/60:.1f}min"
        )
        
        return final_result
    
    def _save_results(self, result: Dict[str, Any], output_file: str):
        """çµæœã®ä¿å­˜"""
        base_path = Path(output_file).with_suffix('')
        
        # JSONå‡ºåŠ›
        json_file = f"{base_path}_segmented.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        
        # CSVå‡ºåŠ›
        csv_file = f"{base_path}_segmented.csv"
        with open(csv_file, 'w', encoding='utf-8') as f:
            f.write("segment_id,start_seconds,end_seconds,duration,text,confidence,speaker\n")
            
            for i, segment in enumerate(result['segments']):
                duration = segment['end'] - segment['start']
                text = segment['text'].replace('"', '""')
                speaker = segment.get('speaker', 'SPEAKER_UNKNOWN')
                confidence = segment.get('confidence', 0.0)
                
                f.write(f"{i},{segment['start']:.3f},{segment['end']:.3f},"
                       f"{duration:.3f},\"{text}\",{confidence:.3f},{speaker}\n")
        
        # SRTå‡ºåŠ›
        srt_file = f"{base_path}_segmented.srt"
        with open(srt_file, 'w', encoding='utf-8') as f:
            for i, segment in enumerate(result['segments']):
                start_time = self._seconds_to_srt_time(segment['start'])
                end_time = self._seconds_to_srt_time(segment['end'])
                
                f.write(f"{i+1}\n")
                f.write(f"{start_time} --> {end_time}\n")
                f.write(f"{segment['text']}\n\n")
        
        self.logger.info(f"Results saved: {json_file}, {csv_file}, {srt_file}")
    
    def _seconds_to_srt_time(self, seconds: float) -> str:
        """ç§’ã‚’SRTæ™‚é–“å½¢å¼ã«å¤‰æ›"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = seconds % 60
        return f"{hours:02d}:{minutes:02d}:{secs:06.3f}".replace('.', ',')

def main():
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Segmented Processor - å¤§å®¹é‡éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«åˆ†å‰²å‡¦ç†",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  # 45åˆ†éŸ³å£°ã®åˆ†å‰²å‡¦ç†
  python -m transcription.segmented_processor input.mp3 -o output --model large-v3
  
  # ã‚«ã‚¹ã‚¿ãƒ ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚µã‚¤ã‚º
  python -m transcription.segmented_processor input.mp3 -o output --segment-size 15
        """
    )
    
    parser.add_argument('audio_file', help='éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    parser.add_argument('-o', '--output', required=True, help='å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹')
    parser.add_argument('--model', default='large-v3-turbo',
                       choices=['tiny', 'base', 'small', 'medium', 'large', 'large-v3', 'large-v3-turbo', 'turbo'],
                       help='Whisperãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º (turbo: 8å€é«˜é€ŸåŒ–ç‰ˆ)')
    parser.add_argument('--language', default='ja', help='è¨€èªã‚³ãƒ¼ãƒ‰')
    parser.add_argument('--segment-size', type=float, default=10.0,
                       help='ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚µã‚¤ã‚ºï¼ˆåˆ†ï¼‰')
    parser.add_argument('--overlap', type=float, default=5.0,
                       help='ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ï¼ˆç§’ï¼‰')
    parser.add_argument('--enable-speaker', action='store_true',
                       help='è©±è€…èªè­˜ã‚’æœ‰åŠ¹åŒ–')
    
    args = parser.parse_args()
    
    # ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼åˆæœŸåŒ–
    processor = SegmentedProcessor(
        segment_minutes=args.segment_size,
        overlap_seconds=args.overlap
    )
    
    try:
        # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
        result = processor.process_file(
            audio_file=args.audio_file,
            output_file=args.output,
            model=args.model,
            language=args.language,
            enable_speaker_recognition=args.enable_speaker
        )
        
        print(f"\nâœ… åˆ†å‰²å‡¦ç†å®Œäº†!")
        print(f"ğŸ“Š çµæœ:")
        print(f"   - ç·ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°: {result['metadata']['total_segments']}")
        print(f"   - å¹³å‡ä¿¡é ¼åº¦: {result['metadata']['average_confidence']:.1%}")
        print(f"   - åˆ†å‰²æ•°: {result['metadata']['segment_count']}")
        print(f"   - å‡¦ç†æ™‚é–“: {result['metadata']['total_processing_time']/60:.1f}åˆ†")
        print(f"   - éŸ³å£°æ™‚é–“: {result['metadata']['total_duration']/60:.1f}åˆ†")
        print(f"   - ãƒ¢ãƒ‡ãƒ«: {result['metadata']['model_used']}")
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸  ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹ä¸­æ–­")
        sys.exit(130)
    except Exception as e:
        print(f"\nâŒ å‡¦ç†å¤±æ•—: {e}")
        sys.exit(1)

if __name__ == '__main__':
    main()