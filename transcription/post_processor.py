"""
Post-processing for transcription results using context and language models.
"""

import re
import unicodedata
from typing import List, Dict, Tuple
import numpy as np


class TranscriptionPostProcessor:
    """Post-process transcription results for better accuracy."""
    
    def __init__(self):
        """Initialize post processor."""
        # Common Japanese word corrections
        self.word_corrections = {
            # Technical terms
            '„Ç∏„Çß„Éç„É¨„Ç§„ÉÜ„Ç£„Éñ': '„Ç∏„Çß„Éç„É¨„Éº„ÉÜ„Ç£„Éñ',
            '„Éó„É≠„É≥„Éó„Éà': '„Éó„É≠„É≥„Éó„Éà',
            '„ÉÅ„É£„ÉÉ„Éà': '„ÉÅ„É£„ÉÉ„Éà',
            '„Çπ„Éó„É¨„ÉÉ„Éà': '„Çπ„Éó„É¨„ÉÉ„Éâ',
            '„Çπ„É©„Ç§„Éà': '„Çπ„É©„Ç§„Éâ',
            '„Ç≥„Ç∑„Éº': '„Ç≥„Ç∑',
            '„Ç≥„ÉÉ„Ç∑„Éº': '„Ç≥„Ç∑',
            
            # Common mishears
            '„Ç∏„Çß„Éü„Å´': 'Gemini',
            '„Ç∏„Çß„Éü„Éã„Éº': 'Gemini',
            '„ÉÅ„Çß„Éü„Éã': 'Gemini',
            '„ÉÅ„É£„ÉÉ„ÉóGPT': 'ChatGPT',
            '„ÉÅ„É£„ÉÉ„Éógpt': 'ChatGPT',
            'DPL': 'DeepL',
            '„Éá„Ç£„Éº„Éö„Éº„É´': 'DeepL',
            'D„Éó„É™„Çµ„Éº„ÉÅ': 'D.B.Research',
            'dbrisarch': 'DB Research',
            
            # Common words
            '„Åã„Åë„Åº„ÅÜ': 'ÂÆ∂Ë®àÁ∞ø',
            '„Åã„Åë„ÅÉ„Åì„ÅÜ': 'ÂÆ∂Ë®àÁ∞ø',
            '„ÉÜ„ÉÉ„Éâ': '„Çø„Çπ„ÇØ',
            '„ÉÜ„ÉÉ„Éâ„É™„Éê„Éº': '„Çø„Çπ„ÇØ',
            '„Éè„É≥„Çø„Éº': '„Éè„É≥„Éâ„É©„Éº',
            'CËâ≤': 'Êé®Ë´ñ',
            '„Çπ„Ç§„É≠„É≥': 'Êé®Ë´ñ',
            'Êú¨Ëñ¨': 'ÁøªË®≥',
            '„Éõ„É≥Ëñ¨': 'ÁøªË®≥',
            '„Éó„É©„É¢„Éõ„É≥ÂΩπ': '„Éó„É©„É¢ÁøªË®≥',
            
            # Numbers and units
            '1ÂÜç': '‰∏ÄÂàá',
            'ÈÅãËª¢„Éû„É≥': 'Êï∞ÂçÅ‰∏á',
            'ÈÅãËñ¨‰∏á': 'Êï∞ÂçÅ‰∏á',
            'ÈÅã„Åπ„Åè„Åæ„Çì': 'Êï∞ÂçÅ‰∏á',
            'ÈÅãËª∏„Éû„É≥„Éà': 'Êï∞ÂçÅ‰∏á',
            'ÁöÑ„Çπ„Çø„Éº„Éà': '„ÉÜ„Ç≠„Çπ„Éà',
            '„Ç¢„ÉÉ„Éó„Çí„ÇØ„É™„Éº„ÉÄ„Éº': '„Ç¢„ÉÉ„Éó„Éª„ÇØ„É™„Ç®„Éº„Çø„Éº',
            
            # Business terms
            '„Éï„É™„Éº„Éä„Éº„Çπ': '„Éï„É™„Éº„É©„É≥„Çπ',
            '„Ç¨„Ç§„Éñ„Å´': 'Â§ñÈÉ®„Å´',
            '„ÅÑ„Åü„Åè„Åô„Çã': 'ÂßîË®ó„Åô„Çã',
            'ÁîüÊàêÂêà': 'ÁîüÊàêAI',
            'ÁîüÊàêÊÑõ': 'ÁîüÊàêAI',
            '„Çª„Çπ„Ç§„É§„Éº': '„Ç®„É≥„Ç∏„Éã„Ç¢',
            'Ê∫ñÂÇô': 'Gemini',
            
            # Locations and names
            '„Åä„Åµ„Çè': 'ÂÉï„ÅØ',
            '„Éä„É™„ÉÑ': '„Å™„Çä„Å§„Å§',
            '„Åä„Åù„Åì„Åï„Çì': '„ÅäÁñ≤„Çå„Åï„Çì',
            '„Åä‰∫°„Åè': 'ÁøªË®≥',
            
            # Time expressions
            '‰∏âËßíÈñì': '3Êó•Èñì',
            '‰∫∫Âêë„Åã„Åó': '2Âπ¥',
            '‰∫∫Âêë„Åã„Åó„Åæ„ÅÑ': '2Âπ¥Ââç',
            '2„ÄÅ3„ÄÅ4„ÄÅ5Âπ¥Ââç': '2„ÄÅ3Âπ¥Ââç',
        }
        
        # Contextual patterns for better accuracy
        self.context_patterns = [
            # Technology context
            (r'(AI|‰∫∫Â∑•Áü•ËÉΩ).*(„Çπ„Çø„Ç∏„É•„Éº|„Çπ„Çø„Ç∏„Ç™)', r'\1„Çπ„Çø„Ç∏„Ç™'),
            (r'(Google|„Ç∞„Éº„Ç∞„É´).*(AI|„Ç®„Éº„Ç¢„Ç§)', r'\1 AI'),
            (r'(„Éû„Éº„ÇØ„ÉÄ„Ç¶„É≥).*(ÂΩ¢Âºè|„Éï„Ç©„Éº„Éû„ÉÉ„Éà)', r'\1ÂΩ¢Âºè'),
            (r'(„Çπ„É©„Ç§„Éâ).*(Âåñ|„Åã)', r'\1Âåñ'),
            (r'(„ÉÅ„É£„ÉÉ„Éà).*(GPT|„Ç∏„Éº„Éî„Éº„ÉÜ„Ç£„Éº)', r'ChatGPT'),
            
            # Business context
            (r'(„Éï„É™„Éº).*(„É©„É≥„Çπ|„Éä„Éº„Çπ)', r'„Éï„É™„Éº„É©„É≥„Çπ'),
            (r'(Ëá™ÂãïÂåñ).*(„Åó„Åü„ÅÑ|„Åó„Åü„Åè„Å™„ÅÑ)', r'\1„Åó„Åü„ÅÑ'),
            (r'(ÈñãÁô∫).*(ÈÄ≤„ÇÅ„Çã|„Åô„Åô„ÇÅ„Çã)', r'\1„ÇíÈÄ≤„ÇÅ„Çã'),
            
            # Numbers and quantities
            (r'(\d+).*(„Çª„É≥„Éà|„ÉØ„Éº„Éâ)', r'\1„ÉØ„Éº„Éâ'),
            (r'(ÈÅã[Ëª¢Ëñ¨Ëª∏]).*(‰∏á|„Éû„É≥)', r'Êï∞ÂçÅ‰∏á'),
            (r'(\d+).*(ÂàÜ|„Å∂„Çì).*(„Åê„Çâ„ÅÑ|„Åè„Çâ„ÅÑ)', r'\1ÂàÜ„Åê„Çâ„ÅÑ'),
        ]
    
    def normalize_text(self, text: str) -> str:
        """Normalize text using Unicode normalization."""
        # NFKC normalization for Japanese text
        normalized = unicodedata.normalize('NFKC', text)
        
        # Remove extra whitespaces
        normalized = re.sub(r'\s+', ' ', normalized).strip()
        
        return normalized
    
    def apply_word_corrections(self, text: str) -> str:
        """Apply word-level corrections."""
        corrected = text
        
        for wrong, correct in self.word_corrections.items():
            # Case-insensitive replacement
            pattern = re.escape(wrong)
            corrected = re.sub(pattern, correct, corrected, flags=re.IGNORECASE)
        
        return corrected
    
    def apply_context_corrections(self, text: str) -> str:
        """Apply context-based corrections."""
        corrected = text
        
        for pattern, replacement in self.context_patterns:
            corrected = re.sub(pattern, replacement, corrected, flags=re.IGNORECASE)
        
        return corrected
    
    def fix_sentence_boundaries(self, text: str) -> str:
        """Fix sentence boundaries and punctuation."""
        # Add periods after common sentence endings
        text = re.sub(r'(„Åß„Åô|„Åæ„Åô|„Åß„Åó„Åü|„Åæ„Åó„Åü)(?![„ÄÇÔºÅÔºü])', r'\1„ÄÇ', text)
        text = re.sub(r'(„Åß„Åô„Å≠|„Åæ„Åô„Å≠|„Åß„Åô„Çà|„Åæ„Åô„Çà)(?![„ÄÇÔºÅÔºü])', r'\1„ÄÇ', text)
        
        # Fix spacing around punctuation
        text = re.sub(r'\s*([„ÄÇÔºÅÔºü])\s*', r'\1 ', text)
        
        # Remove trailing spaces
        text = re.sub(r'\s+$', '', text)
        
        return text
    
    def confidence_based_filtering(self, segments: List[Dict], min_confidence: float = 0.4) -> List[Dict]:
        """Filter segments based on confidence with contextual consideration."""
        filtered = []
        
        for i, segment in enumerate(segments):
            confidence = segment.get('confidence', 0.0)
            text = segment.get('text', '').strip()
            
            # Skip very short, low-confidence segments
            if len(text) <= 2 and confidence < 0.6:
                continue
            
            # Keep high-confidence segments
            if confidence >= min_confidence:
                filtered.append(segment)
                continue
            
            # For medium confidence, check context
            if confidence >= 0.2:
                # Check surrounding context
                context_bonus = 0.0
                
                # Bonus for being surrounded by high-confidence segments
                if i > 0 and segments[i-1].get('confidence', 0) > 0.7:
                    context_bonus += 0.1
                if i < len(segments)-1 and segments[i+1].get('confidence', 0) > 0.7:
                    context_bonus += 0.1
                
                # Bonus for containing common words
                if any(word in text for word in ['„Åß„Åô', '„Åæ„Åô', '„Å®„ÅÑ„ÅÜ', '„ÅÆ„Åß', '„Åã„Çâ']):
                    context_bonus += 0.1
                
                if confidence + context_bonus >= min_confidence:
                    # Update confidence
                    segment = segment.copy()
                    segment['confidence'] = min(1.0, confidence + context_bonus)
                    filtered.append(segment)
        
        return filtered
    
    def merge_short_segments(self, segments: List[Dict], min_duration: float = 2.0) -> List[Dict]:
        """Merge very short segments with adjacent ones."""
        if len(segments) <= 1:
            return segments
        
        merged = []
        i = 0
        
        while i < len(segments):
            current = segments[i].copy()
            duration = current['end_time'] - current['start_time']
            
            # If segment is too short, try to merge with next
            if duration < min_duration and i < len(segments) - 1:
                next_segment = segments[i + 1]
                
                # Merge if gap is small (< 1 second)
                gap = next_segment['start_time'] - current['end_time']
                if gap < 1.0:
                    # Merge segments
                    current['end_time'] = next_segment['end_time']
                    current['text'] = f"{current['text']} {next_segment['text']}"
                    # Average confidence
                    current['confidence'] = (current['confidence'] + next_segment['confidence']) / 2
                    i += 2  # Skip next segment as it's merged
                else:
                    merged.append(current)
                    i += 1
            else:
                merged.append(current)
                i += 1
        
        return merged
    
    def process_transcription(self, segments: List[Dict], preserve_fillers: bool = False) -> List[Dict]:
        """
        Complete post-processing pipeline.
        
        Args:
            segments: List of transcription segments
            preserve_fillers: If True, preserve filler words like „Å™„Çã„Åª„Å©, „Åü„Åó„Åã„Å´
        """
        print("üîß Post-processing transcription...")
        
        processed = []
        
        for segment in segments:
            text = segment.get('text', '')
            
            # Apply all text corrections
            text = self.normalize_text(text)
            text = self.apply_word_corrections(text)
            text = self.apply_context_corrections(text)
            text = self.fix_sentence_boundaries(text)
            
            # Update segment
            processed_segment = segment.copy()
            processed_segment['text'] = text
            processed.append(processed_segment)
        
        print(f"   - Applied corrections to {len(processed)} segments")
        
        # Apply filtering and merging
        processed = self.confidence_based_filtering(processed, min_confidence=0.3)
        print(f"   - Filtered to {len(processed)} segments")
        
        # Skip merging if preserving fillers
        if not preserve_fillers:
            processed = self.merge_short_segments(processed, min_duration=1.5)
            print(f"   - Merged to {len(processed)} segments")
        else:
            print(f"   - Skipped merging to preserve filler words")
        
        print("‚úÖ Post-processing completed")
        
        return processed