"""
Post-processing for transcription results using context and language models.
"""

import re
import unicodedata
from typing import List, Dict, Tuple
import numpy as np


class TranscriptionPostProcessor:
    """Post-process transcription results for better accuracy."""
    
    def __init__(self):
        """Initialize post processor."""
        # Common Japanese word corrections
        self.word_corrections = {
            # Technical terms
            'ã‚¸ã‚§ãƒãƒ¬ã‚¤ãƒ†ã‚£ãƒ–': 'ã‚¸ã‚§ãƒãƒ¬ãƒ¼ãƒ†ã‚£ãƒ–',
            'ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ': 'ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ',
            'ãƒãƒ£ãƒƒãƒˆ': 'ãƒãƒ£ãƒƒãƒˆ',
            'ã‚¹ãƒ—ãƒ¬ãƒƒãƒˆ': 'ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰',
            'ã‚¹ãƒ©ã‚¤ãƒˆ': 'ã‚¹ãƒ©ã‚¤ãƒ‰',
            'ã‚³ã‚·ãƒ¼': 'ã‚³ã‚·',
            'ã‚³ãƒƒã‚·ãƒ¼': 'ã‚³ã‚·',
            
            # Common mishears
            'ã‚¸ã‚§ãƒŸã«': 'Gemini',
            'ã‚¸ã‚§ãƒŸãƒ‹ãƒ¼': 'Gemini',
            'ãƒã‚§ãƒŸãƒ‹': 'Gemini',
            'ãƒãƒ£ãƒƒãƒ—GPT': 'ChatGPT',
            'ãƒãƒ£ãƒƒãƒ—gpt': 'ChatGPT',
            'DPL': 'DeepL',
            'ãƒ‡ã‚£ãƒ¼ãƒšãƒ¼ãƒ«': 'DeepL',
            'Dãƒ—ãƒªã‚µãƒ¼ãƒ': 'D.B.Research',
            'dbrisarch': 'DB Research',
            
            # Common words
            'ã‹ã‘ã¼ã†': 'å®¶è¨ˆç°¿',
            'ã‹ã‘ãƒã“ã†': 'å®¶è¨ˆç°¿',
            'ãƒ†ãƒƒãƒ‰': 'ã‚¿ã‚¹ã‚¯',
            'ãƒ†ãƒƒãƒ‰ãƒªãƒãƒ¼': 'ã‚¿ã‚¹ã‚¯',
            'ãƒãƒ³ã‚¿ãƒ¼': 'ãƒãƒ³ãƒ‰ãƒ©ãƒ¼',
            'Cè‰²': 'æŽ¨è«–',
            'ã‚¹ã‚¤ãƒ­ãƒ³': 'æŽ¨è«–',
            'æœ¬è–¬': 'ç¿»è¨³',
            'ãƒ›ãƒ³è–¬': 'ç¿»è¨³',
            'ãƒ—ãƒ©ãƒ¢ãƒ›ãƒ³å½¹': 'ãƒ—ãƒ©ãƒ¢ç¿»è¨³',
            
            # Numbers and units
            '1å†': 'ä¸€åˆ‡',
            'é‹è»¢ãƒžãƒ³': 'æ•°åä¸‡',
            'é‹è–¬ä¸‡': 'æ•°åä¸‡',
            'é‹ã¹ãã¾ã‚“': 'æ•°åä¸‡',
            'é‹è»¸ãƒžãƒ³ãƒˆ': 'æ•°åä¸‡',
            'çš„ã‚¹ã‚¿ãƒ¼ãƒˆ': 'ãƒ†ã‚­ã‚¹ãƒˆ',
            'ã‚¢ãƒƒãƒ—ã‚’ã‚¯ãƒªãƒ¼ãƒ€ãƒ¼': 'ã‚¢ãƒƒãƒ—ãƒ»ã‚¯ãƒªã‚¨ãƒ¼ã‚¿ãƒ¼',
            
            # Business terms
            'ãƒ•ãƒªãƒ¼ãƒŠãƒ¼ã‚¹': 'ãƒ•ãƒªãƒ¼ãƒ©ãƒ³ã‚¹',
            'ã‚¬ã‚¤ãƒ–ã«': 'å¤–éƒ¨ã«',
            'ã„ãŸãã™ã‚‹': 'å§”è¨—ã™ã‚‹',
            'ç”Ÿæˆåˆ': 'ç”ŸæˆAI',
            'ç”Ÿæˆæ„›': 'ç”ŸæˆAI',
            'ã‚»ã‚¹ã‚¤ãƒ¤ãƒ¼': 'ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢',
            'æº–å‚™': 'Gemini',
            
            # Locations and names
            'ãŠãµã‚': 'åƒ•ã¯',
            'ãƒŠãƒªãƒ„': 'ãªã‚Šã¤ã¤',
            'ãŠãã“ã•ã‚“': 'ãŠç–²ã‚Œã•ã‚“',
            'ãŠäº¡ã': 'ç¿»è¨³',
            
            # Time expressions
            'ä¸‰è§’é–“': '3æ—¥é–“',
            'äººå‘ã‹ã—': '2å¹´',
            'äººå‘ã‹ã—ã¾ã„': '2å¹´å‰',
            '2ã€3ã€4ã€5å¹´å‰': '2ã€3å¹´å‰',
        }
        
        # Contextual patterns for better accuracy
        self.context_patterns = [
            # Technology context
            (r'(AI|äººå·¥çŸ¥èƒ½).*(ã‚¹ã‚¿ã‚¸ãƒ¥ãƒ¼|ã‚¹ã‚¿ã‚¸ã‚ª)', r'\1ã‚¹ã‚¿ã‚¸ã‚ª'),
            (r'(Google|ã‚°ãƒ¼ã‚°ãƒ«).*(AI|ã‚¨ãƒ¼ã‚¢ã‚¤)', r'\1 AI'),
            (r'(ãƒžãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³).*(å½¢å¼|ãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆ)', r'\1å½¢å¼'),
            (r'(ã‚¹ãƒ©ã‚¤ãƒ‰).*(åŒ–|ã‹)', r'\1åŒ–'),
            (r'(ãƒãƒ£ãƒƒãƒˆ).*(GPT|ã‚¸ãƒ¼ãƒ”ãƒ¼ãƒ†ã‚£ãƒ¼)', r'ChatGPT'),
            
            # Business context
            (r'(ãƒ•ãƒªãƒ¼).*(ãƒ©ãƒ³ã‚¹|ãƒŠãƒ¼ã‚¹)', r'ãƒ•ãƒªãƒ¼ãƒ©ãƒ³ã‚¹'),
            (r'(è‡ªå‹•åŒ–).*(ã—ãŸã„|ã—ãŸããªã„)', r'\1ã—ãŸã„'),
            (r'(é–‹ç™º).*(é€²ã‚ã‚‹|ã™ã™ã‚ã‚‹)', r'\1ã‚’é€²ã‚ã‚‹'),
            
            # Numbers and quantities
            (r'(\d+).*(ã‚»ãƒ³ãƒˆ|ãƒ¯ãƒ¼ãƒ‰)', r'\1ãƒ¯ãƒ¼ãƒ‰'),
            (r'(é‹[è»¢è–¬è»¸]).*(ä¸‡|ãƒžãƒ³)', r'æ•°åä¸‡'),
            (r'(\d+).*(åˆ†|ã¶ã‚“).*(ãã‚‰ã„|ãã‚‰ã„)', r'\1åˆ†ãã‚‰ã„'),
        ]
    
    def normalize_text(self, text: str) -> str:
        """Normalize text using Unicode normalization."""
        # NFKC normalization for Japanese text
        normalized = unicodedata.normalize('NFKC', text)
        
        # Remove extra whitespaces
        normalized = re.sub(r'\s+', ' ', normalized).strip()
        
        return normalized
    
    def apply_word_corrections(self, text: str) -> str:
        """Apply word-level corrections."""
        corrected = text
        
        for wrong, correct in self.word_corrections.items():
            # Case-insensitive replacement
            pattern = re.escape(wrong)
            corrected = re.sub(pattern, correct, corrected, flags=re.IGNORECASE)
        
        return corrected
    
    def apply_context_corrections(self, text: str) -> str:
        """Apply context-based corrections."""
        corrected = text
        
        for pattern, replacement in self.context_patterns:
            corrected = re.sub(pattern, replacement, corrected, flags=re.IGNORECASE)
        
        return corrected
    
    def fix_sentence_boundaries(self, text: str) -> str:
        """Fix sentence boundaries and punctuation."""
        # Add periods after common sentence endings
        text = re.sub(r'(ã§ã™|ã¾ã™|ã§ã—ãŸ|ã¾ã—ãŸ)(?![ã€‚ï¼ï¼Ÿ])', r'\1ã€‚', text)
        text = re.sub(r'(ã§ã™ã­|ã¾ã™ã­|ã§ã™ã‚ˆ|ã¾ã™ã‚ˆ)(?![ã€‚ï¼ï¼Ÿ])', r'\1ã€‚', text)
        
        # Fix spacing around punctuation
        text = re.sub(r'\s*([ã€‚ï¼ï¼Ÿ])\s*', r'\1 ', text)
        
        # Remove trailing spaces
        text = re.sub(r'\s+$', '', text)
        
        return text
    
    def confidence_based_filtering(self, segments: List[Dict], min_confidence: float = 0.4) -> List[Dict]:
        """Filter segments based on confidence with contextual consideration."""
        filtered = []
        
        for i, segment in enumerate(segments):
            confidence = segment.get('confidence', 0.0)
            text = segment.get('text', '').strip()
            
            # Skip very short, low-confidence segments
            if len(text) <= 2 and confidence < 0.6:
                continue
            
            # Keep high-confidence segments
            if confidence >= min_confidence:
                filtered.append(segment)
                continue
            
            # For medium confidence, check context
            if confidence >= 0.2:
                # Check surrounding context
                context_bonus = 0.0
                
                # Bonus for being surrounded by high-confidence segments
                if i > 0 and segments[i-1].get('confidence', 0) > 0.7:
                    context_bonus += 0.1
                if i < len(segments)-1 and segments[i+1].get('confidence', 0) > 0.7:
                    context_bonus += 0.1
                
                # Bonus for containing common words
                if any(word in text for word in ['ã§ã™', 'ã¾ã™', 'ã¨ã„ã†', 'ã®ã§', 'ã‹ã‚‰']):
                    context_bonus += 0.1
                
                if confidence + context_bonus >= min_confidence:
                    # Update confidence
                    segment = segment.copy()
                    segment['confidence'] = min(1.0, confidence + context_bonus)
                    filtered.append(segment)
        
        return filtered
    
    def merge_short_segments(self, segments: List[Dict], min_duration: float = 2.0) -> List[Dict]:
        """Merge very short segments with adjacent ones."""
        if len(segments) <= 1:
            return segments
        
        merged = []
        i = 0
        
        while i < len(segments):
            current = segments[i].copy()
            duration = current['end_time'] - current['start_time']
            
            # If segment is too short, try to merge with next
            if duration < min_duration and i < len(segments) - 1:
                next_segment = segments[i + 1]
                
                # Merge if gap is small (< 1 second)
                gap = next_segment['start_time'] - current['end_time']
                if gap < 1.0:
                    # Merge segments
                    current['end_time'] = next_segment['end_time']
                    current['text'] = f"{current['text']} {next_segment['text']}"
                    # Average confidence
                    current['confidence'] = (current['confidence'] + next_segment['confidence']) / 2
                    i += 2  # Skip next segment as it's merged
                else:
                    merged.append(current)
                    i += 1
            else:
                merged.append(current)
                i += 1
        
        return merged
    
    def process_transcription(self, segments: List[Dict]) -> List[Dict]:
        """
        Complete post-processing pipeline.
        """
        print("ðŸ”§ Post-processing transcription...")
        
        processed = []
        
        for segment in segments:
            text = segment.get('text', '')
            
            # Apply all text corrections
            text = self.normalize_text(text)
            text = self.apply_word_corrections(text)
            text = self.apply_context_corrections(text)
            text = self.fix_sentence_boundaries(text)
            
            # Update segment
            processed_segment = segment.copy()
            processed_segment['text'] = text
            processed.append(processed_segment)
        
        print(f"   - Applied corrections to {len(processed)} segments")
        
        # Apply filtering and merging
        processed = self.confidence_based_filtering(processed, min_confidence=0.3)
        print(f"   - Filtered to {len(processed)} segments")
        
        processed = self.merge_short_segments(processed, min_duration=1.5)
        print(f"   - Merged to {len(processed)} segments")
        
        print("âœ… Post-processing completed")
        
        return processed